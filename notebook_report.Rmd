---
title: 'Reporte Kaggle Cellclass'
author: "Martí Gelabert Gómez"
date: "28/12/2021"
output: 
  html_notebook: 
    toc: yes
    toc_depth: 5
    number_sections: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{python, echo=FALSE}
# Funciones y imports de la práctica
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn import linear_model
from os.path import isfile, join
import os
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

import seaborn as sns

# t puede ser train o test
def generate_data(t):
  c=['Class']
  #c=['Aspect ratio','Elongation','FD1','Class']
  
  if(t == 'train'):
    color=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/color_'+t+'.csv',sep=',',decimal=',')
    shape=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/shape_'+t+'.csv',sep=',',decimal=',')
    texture=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/texture_'+t+'.csv',sep=',',decimal=',')
    info=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/info_'+t+'.csv',sep=';',decimal=',') # Estos tienen ;
    
    #frames = [  color,shape,texture]
    frames = [shape, texture]
    result = pd.concat(frames,axis=1)
    
    x = pd.DataFrame(result.iloc[:, 0:122])
    _ = pd.DataFrame(result.iloc[:, -1])
    
    y = pd.DataFrame(info['class'])
    for i in c:
      x.drop(i, axis=1, inplace=True)
    #y = LabelEncoder().fit_transform(y['Class'])
  else:
    color=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/color_'+t+'.csv',sep=';',decimal=',')
    shape=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/shape_'+t+'.csv',sep=';',decimal=',')
    texture=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/texture_'+t+'.csv',sep=';',decimal=',')
    #info=pd.read_csv(filepath_or_buffer='data/cellclass/'+t+'/info_'+t+'.csv',sep=';',decimal=',') # Estos tienen ;
    
    frames = [shape, texture]
    result = pd.concat(frames,axis=1)
    
    x = pd.DataFrame(result.iloc[:, 0:122])
    y = 0
    for i in c:
      x.drop(i, axis=1, inplace=True)
  return x,y


def print_csv(y_pred,s):
  info=pd.read_csv(filepath_or_buffer='data/cellclass/test/info_test.csv',sep=';',decimal=',')
  cosa = {'class': y_pred}
  frames=[info,pd.DataFrame(cosa)]
  a=pd.concat(frames,axis=1)
  pd.DataFrame(a).to_csv('SMVGRID/'+s+'.csv',index = False)

def print_csv_current(y_pred,s):
  info=pd.read_csv(filepath_or_buffer='data/cellclass/test/info_test.csv',sep=';',decimal=',')
  cosa = {'class': y_pred}
  frames=[info,pd.DataFrame(cosa)]
  a=pd.concat(frames,axis=1)
  pd.DataFrame(a).to_csv(s+'.csv',index = False)

def trimm_correlated(df_in, threshold):
  df_corr = df_in.corr(method='pearson', min_periods=1)
  df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > threshold).any()
  un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index
  df_out = df_in[un_corr_idx]
  return df_out

def plot_conf_mtrx(y_test, y_pred):
  cf_matrix= confusion_matrix(y_test, y_pred)
  plt.clf()#para hacer el flush
  ax = sns.heatmap(cf_matrix, annot=True, cmap='Reds', cbar=False)
  
  ax.set_title('Matriz de confusión\n\n');
  
  plt.show()
```

# Introducción
Esta práctica consiste en analizar y probar diferentes modelos de aprendizaje con los datasets que se han públicado desde Kaggle. Los datos propuestos se encuentran dividos en tres ficheros haciendo referencia cada uno a atributos diferentes de las celulas que queremos clasificar, dentro de estos archivos tenemos valores que referencian a el **color**, la **forma** y su **textura**.

El resultado de la clasificación se debe comprobar subiendo los datos a una competición de Kaggle creada para la práctica.

Aparentemente los datos se han tomado desde el mismo microscopio, por lo que no debería de haber ningún problema a la hora de coger las muestras de test y de train (presuntamente), en primer lugar podríamos empezar a descartar algunos elementos como por ejemplo el color, ya que no debería de ser importante con respecto a diferentes elementos como su forma o textura. Generalmente a la hora de tratar con imagenes se deberían de reducir sus dimensiones (cosa que no se hace en esta practica), por lo que probablemente se reduciria tanto la resolución de la imagen como la cantidad de información de los canales de color (si quisieramos tener los elementos más caracteristicos claro) pero en lugar de especular, emplearemos los diferentes modelos de aprendizaje que se han podido ver durante el curso (a excepcción de los multilayer) para demostrar nuestras hipotesis.  

El Github donde se encuentra esta documentación y el rmd esta en este [link](https://github.com/martigelabert/Cellclass_kaggle).

# Analisis general de datos

Importaremos los datos en formato pandas para obtener una visión general de nuestros dataset y se visualizarán uno a uno los archivos csv a emplear.

<div class="alert alert-block alert-info">
<b>Nota:</b> No sé comentará el uso concreto de todos los atributos, ya que estos están comentados en un paper público de donde se han extraido estos datos y acabariamos con un reporte mucho más largo innecesariamente. 
</div>

## Color

Comenzaremos por el dataframe de color:

```{python echo=FALSE}
color=pd.read_csv(filepath_or_buffer='data/cellclass/train/color_train.csv',sep=',',decimal=',')
print(color)

```

Son 19 columnas relacionadas con el color obtenido desde las imagens que también están disponibles desde el directorio de kaggle, en este caso no se emplearían para esta práctica ya que no se nos pide nada relacionado con entrenamiento mediante imagenes.

### ¿Porque no emplear color en este caso?

Como se ha mencionado antes, el color de las imagenes no debería de ser uno de los factores para predecir el tipo de clase de nuesta celula, por varias razones, como por ejemplo que no estamos en un problema de procesamiento de imagenes, y aunque lo estuvieramos precisamente el color no sería uno de los factores que nos permitiría obtener información importante como los **hard edges** de las formas.

En si mismo, el concepto de la percepcción del color y de su tratamiento es complejo y computacionalmente costoso, y si aún así se decidiera emplear el color para procesamiento de imagenes, seguiriamos teniendo un problema de complegidad de muy grande, ya que al tener que estar tratando más dimensiones y probablemente también tener en cuenta la manera en la que se perciben, aumentariamos tanto la dificultad de entrenamiento como el tratamiento de los datos.

Otra razón por la que quizá emplear el color no sería buena idea, sería por la varianza con respecto a las diferentes gamas de color que un dispositivo es capaz de distinguir y procesar, lo que implica que si entrenamos con un tipo de images que se han tomado con un dispositivo **A** que es mucho más sensible a tonos rojizos, quizá por ello, nuestro modelo que se ha entrenado con los datos A, al generalizar con información de color obtenida desde un dispositivo **B** que tiene mayor sensibilidad a tonos azulados, podría no clasificar bien nuestra muestra, pese a que las caracteristicas pudieran ser idones para la clasificación, pero debido al ruido que nos aporta el factor del color no lo hemos podido predecir correctamente. 

Por estas razones son las que se reducen las dimensiones de color a **grayscale**, donde por lo menos si la imagen no tiene una resolución elevada (después de haber sido tratada) el problema debería de volverse computacionalmente algo más accesible para su resolución.

Aún así, las primerás predicciones que se submitearón para esta práctica, los modelo empleados estabán entrenados con estos atributos, sin embargo, posteriormente se elimino completamente del conjunto de aprendizaje y se mejorarón considerablemente los resultados obtenidos, lo que confirma nuestra teoría anterior sobre que el color no es un dato relevante para nosotros en este caso.

## Shape
En este archivo csv tenemos atributos relacionados con la forma de nuestra celula, es decir varaibles haciendo referencia a sus dimensiones, a su perimetro, area, rango con respecto a los ejes, etc. Estos datos aparentemente, ya contienen más información sobre los elementos más caracteristicos para proporcionar mejores resultados de clasificación, que empleando los atributos de color.

```{python echo=FALSE}
shape=pd.read_csv(filepath_or_buffer='data/cellclass/train/shape_train.csv',sep=',',decimal=',')
print(shape)
```

## Texture
Aquí nuestro csv contiene información sobre el tipo de **textura** (extraidas de los histogramas  de la escala de grises de las imagenes) de nuestras muestras .

```{python echo=FALSE}
texture=pd.read_csv(filepath_or_buffer='data/cellclass/train/texture_train.csv',sep=',',decimal=',')
print(texture)
```

# Modelos de clasificación

Los **modelos de clasificación** que se han probado son los siguientes:

* SVM: concretamente el SVMClassifier, si le añadimos un buen tratamiento de datos se podrían obtener resultados que generalizasen bastante bien nuestros datos, además de que con todas las opciones de configuración de tipo de kernel tendremos mucho margen de configuración.

* Random Forest: Una versión que parte de arboles de decisión, más flexible que estos y pueden llegar a funcionar muy bien si se escogen correctamente los datos caracteristicos con los que sesgar nuestras muestras.

* Arboles de decisión: Quizá menos flexibles que los random forest, tienden el riesgo de acostumbrarse mucho a los datos de entrenamiento y causar overfitting (sin emplear pruning) pero no implica que no nos puedan llegar a servir.

## Grid Search
Para optimizar la selección de **hiper-parametros** y obtener los mejors resultados posibles, emplearemos los modulos de grid-search que nos ofrece la librería de sklearn.

# Tratamiento de datos y experimentación
## Reducción de dimensionalidad de los datos
Reduccir la dimensionalidad de los datos de nuestro es una buena manera de quedarmos con las **caracteristicas esenciales** que queremos distinguir para nuestros entrenamientos. El hecho de estar quitando ruido implica que hacemos el problema más sencillo a nivel computacional, además estamos evitando que nuestro modelo tenga ciertas tendencias causadas por todas estás variables correlacionadas, lo que generalmente repercute en mejores resultados de predicción.

En el caso donde de emplear decision trees o derivados, es cierto que reducir las dimensionalidades para eliminar la multicolinealidad no debería de afectar a nuestros resultados, ya que son inmunes debido a que estos a la hora de dividirse se **emplea solo una** de las dos caracteristicas correlacionadas.

De todas maneras, otros algoritmos como la **regresión lineal** o la **regresión logistica** no son inmunes a esta, por lo que para evitar problemas, se puede afrontar de varias maneras, una de ellas **eliminando a manualmente** una de las columnas que tuvieran una correlación más alta que la de nuestro umbral establecido, de esta manera solo nos quedaríamos con columnas que tuvierán una **correlación menor a nuestro umbral**. Otra forma sería empleando *Principal component analysis* (PCA), que nos permitiría obtener las caracteristicas más importantes de nuestros datos.

Ambas opcciones son totalmente validas, por lo que podríamos probar a emplear las dos para comparar resultados.

### PCA
Antes de aplicar PCA tendremos que scalar nuestros datos y gracias a que sklearn tiene ya modulos preparados para esto, solo tendremos que importar de *preprocesing* lo que vayamos a necesitar:

```{python}
# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)
```

Una vez tenemos nuestros datos **separados** en nuestro conjunto de datos de entrenamiento y test, podemos proceder a escalar los datos. **Nunca** debemos escalar nuestro conjunto de datos antes de dividirlo, ya que si lo hicieramos **estariamos creando una tendencia en nuestros datos**, además de estar perdiendo información.

La manera correcta de escalar es calculando nuestra media y varianza empleando nuestro conjunto de training y una vez calculado normalizariamos empleando la misma media y varianza de **training**. 

No tendría sentido escalar de forma en que cada conjunto propio escalase en función de sus datos ya que así estaríamos creando también tendencias en nuestro modelo.

Aquí el escalado de los datos es muy importante ya que en este caso, el PCA lo que va a buscar son las caracteristicas con **varianza máxima**, si por algún motivo no normalizasemos podríamos tener valores muy altos (con una alta varianza) que provocaría una tendencia hacía estas caractericticas con estos valores.

```{python}
scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)
```

Con los datos normalizados y centrados pasaremos a tener una desviación estandar (de cada columna) de 1, como podremos ver a continuación:

```{python}
pd.DataFrame(X_train,columns=[x.head()]).std()
```

Ahora solo tendremos que generar un objeto de PCA y realizar los calculos de PCs empleando la función *fit* sobre los datos de entrenamiento ya escalados, por lo que se ha comentado previamente.

```{python}
pca = PCA(random_state=33)
pca.fit(X_train)

X_test= pca.transform(X_test)
X_train=pca.transform(X_train)
```

Al tener el objecto PCA ya con los calculos de ajustes realizados y las transformaciones aplicadas, estaría bien poder analizar el **scree plot** de este, por cuestión de visibilidad solo mostraremos el % de varianza descrito por los primeros 10 componentes:

```{python echo=FALSE}
# código extraido de https://github.com/StatQuest/pca_demo/blob/master/pca_demo.py
#The following code constructs the Scree plot
per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)[0:10]
labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]
 
plt.bar(x=range(1,len(per_var)+1), height=per_var)#, tick_label=labels)
plt.ylabel('Percentage of Explained Variance')
plt.xlabel('Principal Component')
plt.title('Scree Plot')
plt.show()

```

Vemos que algo más del 40\% de toda nuestra **varianza** esta descrita solo por nuestro primer componente principal, aproxmiadamente un 17\% esta descrita por la segunda, y así un par de componentes más (hasta PC10 aproximadamente) que nos permiten describir un % a tener en cuenta de nuestra varianza, con esto **deberiamos poder representar los datos originales de nuestro dataset**.   

Y ahora es simplemente cuestión de encontrar un buen clasificador y ajustar la varianza descrita por nuestras componentes principales hasta obtener unos resultados.

```{python include=FALSE}
pca_var=.95 # jugaremos con esta variable
X_train=np.array(X_train).reshape(len(X_train),-1)
y_train=np.array(y_train).ravel()

X_test=np.array(X_test).reshape(len(X_test),-1)
y_test=np.array(y_test).ravel()

pca = PCA(pca_var)
pca.fit(X_train)
```

#### SVM

Emplearemos las componentes de PCA que nos permitan reconocer un 95% de la varianza:

```{python include=FALSE}
# En las siguientes ejecuciones no se mostrará el código por razones de estetica
pca_var=.95

# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

pca = PCA(pca_var)
pca.fit(X_train)

X_test= pca.transform(X_test)
X_train=pca.transform(X_train)

```

```{python echo=TRUE}

param_grid = {'C': [0.01, 0.1, 0.5, 1, 10,20,15,50,60,70,100,200,300], 
              'gamma': [1, 0.75, 0.65,0.6,0.5,0.35,0.25, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf','poly','linear'],
              'decision_function_shape':['ovo', 'ovr']
             }

grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)
grid.fit(X_train, y_train)
best_params = grid.best_params_

print(f"Best params: {best_params}")

svm_clf = SVC(**best_params)
svm_clf.fit(X_train, y_train)

y_pred=svm_clf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```

Aparetemente tenemos unos resultados muy buenos con una *accuracy* y un *f1-score* prácticamente perfectos, pero sin embargo cuando vamos a kaggle y realizamos el submit del los datos obtenemos una score de 0.43, aparentemente podríamos estar acostumbrandonos demasiado a nuestros datos y al generalizar con nuevos elementos estuvieramos no generalizando bien, el inconveniente es que pese a reajustar parametros y métodos de clasificación en las ejecuciones posteriores los resultados no mejoran significativamente lo que parece algo extraño.

<div class="alert alert-block alert-info">
<b>Nota:</b> Para calcular la score de Kaggle empleamos los datos que se encuentran en la carpeta de test, y tanto el accuray como el f1-score provienen del subvonjunto del conjunto de train. 
</div>

```{python eval=FALSE, include=FALSE}
plot_conf_mtrx(y_test, y_pred)
```

Veamos que podríamos obtener si empleasemos un 90% de la varianza descrita.

```{python include=FALSE}
pca_var=.9

# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

pca = PCA(pca_var)
pca.fit(X_train)

X_test= pca.transform(X_test)
X_train=pca.transform(X_train)

grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)
grid.fit(X_train, y_train)
best_params = grid.best_params_



svm_clf = SVC(**best_params)
svm_clf.fit(X_train, y_train)
```
```{python echo=FALSE}
print(f"Best params: {best_params}")
y_pred=svm_clf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```

Aquí obtenemos resultados ligeramente peores que en la prueba anterior, pero por lo visto logramos incrementar ligeramente nuestra score en kaggle llegando a un 0.63069. Es posible que haya mejorado debido a que logramos generalizar mejor nuestros datos, esto provablemente provoque esos resultados "inferiores" en nuestro reporte ya que no estamos **overfitteando** nuestro modelo o por lo menos no tanto.

#### SVM empleando Stocastic Gradient Descent

Para observar más resultados empleando svm observaremos si las ventajas de emplear *SGD* repercuten en nuestros resultados, así que aplicaremos un PCA que describa una varianza al 90%:

```{python}
pca_var=.9

# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

pca = PCA(pca_var)
pca.fit(X_train)

X_test= pca.transform(X_test)
X_train=pca.transform(X_train)
```
```{python}

SGDClf = linear_model.SGDClassifier(max_iter = 1000, tol=1e-3,penalty = "elasticnet",loss='hinge')
SGDClf.fit(X_train, y_train)

y_pred=SGDClf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))

```

En este caso volvemos a obtener unos resultados muy buenos y somos capaces de predecir muy bien nuestro conjunto de test, pero en kaggle obtenemos una score de 0.57902, algo extraño que podamos modelar tan bien nuestro conjunto de test pero sin embargo no podamos superar casi el 0.6 de score.

```{python eval=FALSE, include=FALSE}
# Dropear dentro de un fichero
X,_=generate_data('test')
X=scaler.transform(X)

X=pca.transform(X)

y_pred_submit=svm_clf.predict(X)

print_csv_current(y_pred_submit,'SGD_elasticnet_hinge_1000iter')
```

### Reducción de dimensiónes manualmente

Ahora que hemos podido ver como se comporta el pca con estos datos, podríamos intentar eliminar nosotros manualmente alguna de las columnas que provocasen fuertes tendencias por la multicolinealidad. Volveremos a emplear el SVM con el grid Search para comparar los resultados que podemos llegar a obtener y ver si realmente podemos mejorar los resultados. 

Empezaremos por eliminar columnas con una correlación mayor a la del 75% para ver si los resultados son similares o mejores a los obtenidos previamente

```{python}
min_correlation=.75

from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.01, 0.1, 0.5, 1, 10,20,15,50,60,70,100,200,300], 
              'gamma': [1, 0.75, 0.65,0.6,0.5,0.35,0.25, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf','poly', 'linear'],
              'decision_function_shape':['ovo', 'ovr']
             }

# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

x=trimm_correlated(x,min_correlation)
keep=list(x) # lista de las columnas que se mantienen en nuestro dataframe

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)
grid.fit(X_train, y_train)
best_params = grid.best_params_
print(f"Best params: {best_params}")

svm_clf = SVC(**best_params)
svm_clf.fit(X_train, y_train)

y_pred=svm_clf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```
```{python eval=FALSE, include=FALSE}
X,_=generate_data('test')
X = X[X.columns.intersection(keep)]
X=scaler.transform(X)
y_pred_submit=svm_clf.predict(X)
print_csv(y_pred_submit,'SMVGRID_R075.csv')
```

Volvemos a obteber resultados muy buenos sobre nuestro conjunto de test pero volvemos a obtener unos resultados similares en kaggle, obteniendo esta vez un 0.54103. Es posible que eliminando las columnas con más de un 75% de correlación estemos reduciendo demasiado poco nuestro número de columnas o que simplemente nuestro conjunto de predicción de kaggle este desvalanceado.

Esta vez reduciremos el número de correlación mínimo a un 60% para ver si el número de columnas a eliminar puede ser el problema, en esta sección solo se mostrará el output del código, ya que es identico al que acabamos de ejecutar

```{python echo=FALSE}
min_correlation=.60

from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.01, 0.1, 0.5, 1, 10,20,15,50,60,70,100,200,300], 
              'gamma': [1, 0.75, 0.65,0.6,0.5,0.35,0.25, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf','poly', 'linear'],
              'decision_function_shape':['ovo', 'ovr']
             }

# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

x=trimm_correlated(x,min_correlation)
keep=list(x) # lista de las columnas que se mantienen en nuestro dataframe

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)
grid.fit(X_train, y_train)
best_params = grid.best_params_
print(f"Best params: {best_params}")

svm_clf = SVC(**best_params)
svm_clf.fit(X_train, y_train)

y_pred=svm_clf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```
```{python eval=FALSE, include=FALSE}
X,_=generate_data('test')
X = X[X.columns.intersection(keep)]
X=scaler.transform(X)
y_pred_submit=svm_clf.predict(X)
print_csv(y_pred_submit,'SMVGRID_R06')
```

Con estos últimos resultaos podemos llevar una grata sorpresa, pese a obtener un accuracy y unos f1-score menores a los del caso anterior, obtenemos un score de 0.65197 en kaggle, el mejor hasta ahora, con estos resultadaos podemos suponer que sesgando aún más las columnas esenciales con menos correlación se generaliza cada vez mejor. 

Como última prueba eliminaremos todas las columnas que superen un 50% de correlación para ver si el modelo puede llegar a mejorar.

```{python eval=FALSE, include=FALSE}
min_correlation=.50

from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.01, 0.1, 0.5, 1, 10,20,15,50,60,70,100,200,300], 
              'gamma': [1, 0.75, 0.65,0.6,0.5,0.35,0.25, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf','poly', 'linear'],
              'decision_function_shape':['ovo', 'ovr']
             }

# Generamos nuestro dataframe con todas las tablas concatenadas
x,y=generate_data('train')

x=trimm_correlated(x,min_correlation)
keep=list(x) # lista de las columnas que se mantienen en nuestro dataframe

# Dividimos nuestro set de entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_test)
X_test= scaler.transform(X_test)
X_train=scaler.transform(X_train)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)
grid.fit(X_train, y_train)
best_params = grid.best_params_
print(f"Best params: {best_params}")

svm_clf = SVC(**best_params)
svm_clf.fit(X_train, y_train)

y_pred=svm_clf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```
```{python eval=FALSE, include=FALSE}
X,_=generate_data('test')
X = X[X.columns.intersection(keep)]
X=scaler.transform(X)
y_pred_submit=svm_clf.predict(X)
print_csv(y_pred_submit,'SMVGRID_R05')
```

Pero aparentemente al reducir tanto las caracteristicas de los datos, obtenemos un peor resultado tanto en accuracy como en la score de kaggle, obteniendo un 0.89 y un 0.45896 respectivamente, esto puede deverse a que estamos eliminando columans que tienen relevancia en la caracterización de los datos por lo que al perderlos el modelo es incapaz de generalizar correctamente. Sin embargo el tener una accuracy casi perfecta en el test vuelve a ser extraño que en el conjunto de submit tengamos un resultado tan inferior en comparación.  

## Decision Trees y derivados

Gracias a la propia naturaleza que forma a los propios algoritmos de arbol de decisión y derivados como el random forest, estos son inmunes a la alta correlación de los datos y los posibles casos de multicolinealidad como se ha comentado previamente. En teoria el uso de herramientas como el PCA o otra forma de reducción de dimensiones no debería de afectar mucho a nuestros resultados y podríamos llegar a tener el mismo rendimiento. Sobre la práctica generalmente nada suele ser tan ideal y puede depender de como tus datos permitan que se rompa el modelo o no.

En este caso para no alargar la fase de experimentación, no se aplicara ningún tipo de reducción de dimensionalidad con estos algoritmos.

### Decision Trees

```{python}
param_grid = {
        'max_features':['auto', 'sqrt', 'log2'],
        'max_leaf_nodes':[2,3,4,5,6,12]
         }

x,y=generate_data('train')

# Tractament de les dades: Separació i estandaritzat
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

grid = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, cv= 5,refit=True)
grid.fit(X_train, y_train)

best_params = grid.best_params_
print(f"Best params: {best_params}")
s=f"Best params: {best_params}"

rf = RandomForestClassifier(**best_params)
#SVC(**best_params)
rf.fit(X_train, y_train)

y_pred=rf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```
```{python eval=FALSE, include=FALSE}

# Dropear dentro de un fichero
X,_=generate_data('test')
X=scaler.transform(X)

y_pred_submit=rf.predict(X)
print_csv(y_pred_submit,'decision_trees')
```

Los resultados de los arboles de decisión no son muy buenos en las scores de kaggle (0.18 la peor hasta ahora), aún así la accuracy es muy buena en nuestro conjunto de testeo, pero claramente no logra generalizar del todo con los datos del concurso, esto es debido a la propia naturaleza de los arboles de decisión, por eso existen versiones que permiten darle algo más de flexibilidad a la hora de clasificar nuestras clases como por ejemplo los **random forest**.

### Random Forest
```{python}
param_grid = {
        'n_estimators': [10,50,100,200,250,300,350],
        'max_features': ['auto', 'sqrt', 'log2'],
        'max_depth' : [4,5,6,7,8],
        'criterion' :['gini', 'entropy']
             }

x,y=generate_data('train')

# Tractament de les dades: Separació i estandaritzat
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=33)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

y_train=np.array(y_train).ravel()
y_test=np.array(y_test).ravel()

grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv= 5,refit=True)
grid.fit(X_train, y_train)

best_params = grid.best_params_
print(f"Best params: {best_params}")
s=f"Best params: {best_params}"

rf = RandomForestClassifier(**best_params)
#SVC(**best_params)
rf.fit(X_train, y_train)

y_pred=rf.predict(X_test)
print(sklearn.metrics.classification_report(y_test, y_pred))
```

```{python eval=FALSE, include=FALSE}

# Dropear dentro de un fichero
X,_=generate_data('test')
X=scaler.transform(X)

y_pred_submit=rf.predict(X)
print_csv(y_pred_submit,'rft_grid')
```

El random forest nos da un score de 0.3237 en kaggle, dandonos un resultado superior al de decision trees, pero una vez más vemos las carencias de este tipo de algoritmos ya que para poder generalizar bien se requiere de muchas muestras.

# Resultados obtenidos
```{r echo=FALSE}
read.csv(file = 'outputs_tabla.csv')
```
No se han incluido todos los test que se han realizado para la práctica, debido a que además de que alargarian demasiado el reporte se ha considerado que no aportarían información útil, más allá de los resultados números, pese a esto podemos ver una clara victoria para el clasificador del SVM, no obstante como ya se a comentado en los resultados anteriores, desde luego no es normal que podamos modelizar tan bien nuestro conjunto de train y que al tratar de predecir nuestro test real tengamos resultados bastante peores, por lo que es una posibilidad que exista algún tipo de problema con este último o simplemente que nuestra score aumentará al emplear todo el conjunto de kaggle en lugar de un subconjunto.

# Conclusiones

El estudiar diferentes algoritmos de aprendizaje automatico han permitido la realización de esta documentación y a dado pie a un pequeño trabajo de experimentación a veces a base de prueba y error, pero sobretodo se ha conseguido aplicar parte de los algoritmos vistos en clase en un entorno de datos "reales", que pese a que sobre el campo teorico pudiesemos pensar que un modelo no llegase a generalizar bien, a base de testeos se ha podido comprobar que en la práctica la experimentación es lo que más peso puede llegar a tener para comprobar el rendimiento que es capaz de tener un modelo. 





